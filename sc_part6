from sklearn.model_selection import ParameterGrid, KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tqdm import tqdm
import numpy as np

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

# Convert param_grid to a list of parameter combinations
param_combinations = list(ParameterGrid(param_grid))

# Initialize variables to store the best score and parameters
best_score = float('inf')
best_params = None
best_model = None

# Progress bar for parameter combinations
progress_bar = tqdm(total=len(param_combinations))

# Loop through each parameter combination
for params in param_combinations:
    # Initialize the model with current parameters
    model = RandomForestRegressor(random_state=42, **params)

    # Perform cross-validation and calculate mean score
    scores = []
    for train_idx, valid_idx in KFold(n_splits=2, shuffle=True, random_state=42).split(X_train_sc_tfidf):  # Reduced to 2 folds
        model.fit(X_train_sc_tfidf[train_idx], y_train_sc[train_idx])
        preds = model.predict(X_train_sc_tfidf[valid_idx])
        score = mean_squared_error(y_train_sc[valid_idx], preds)
        scores.append(score)

    mean_score = np.mean(scores)

    # Update the best score and parameters if current score is better
    if mean_score < best_score:
        best_score = mean_score
        best_params = params
        best_model = model

    # Update the progress bar
    progress_bar.update(1)

progress_bar.close()

# Print the best parameters and score
print("Best parameters found: ", best_params)
print("Best cross-validation score: ", best_score)
